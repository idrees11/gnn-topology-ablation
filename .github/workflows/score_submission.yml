name: Score Submissions

on:
  pull_request:
    branches: [main]       # Trigger workflow on PRs targeting main
  push:
    branches: [main]       # Trigger workflow on push to main
  workflow_dispatch:       # Allows manual trigger from GitHub UI

permissions:
  contents: write           # Needed for pushing leaderboard changes if necessary

jobs:
  scoring:
    runs-on: ubuntu-latest

    steps:
      # ----------------------------
      # 1. Checkout repository
      # ----------------------------
      - name: Checkout repository
        uses: actions/checkout@v3

      # ----------------------------
      # 2. Set up Python environment
      # ----------------------------
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.11

      # ----------------------------
      # 3. Install dependencies
      # ----------------------------
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas scikit-learn

      # ----------------------------
      # 4. Ensure submissions folder exists
      # ----------------------------
      - name: Ensure submissions folder exists
        run: mkdir -p submissions

      # ----------------------------
      # 5. Create dummy submission if empty (PR-safe)
      # ----------------------------
      - name: Create dummy submission if empty
        run: |
          if [ -z "$(ls -A submissions 2>/dev/null)" ]; then
            echo "graph_index,label" > submissions/dummy.csv
            echo "0,0" >> submissions/dummy.csv
            echo "Created dummy submission."
          fi

      # ----------------------------
      # 6. Run scoring script (compute F1 scores)
      # ----------------------------
      - name: Run scoring script
        env:
          TEST_LABELS_B64: ${{ secrets.TEST_LABELS_B64 }}
        run: |
          echo "Running scoring script..."
          python scoring_script.py || echo "Scoring skipped or failed"

      # ----------------------------
      # 7. Run leaderboard system to update leaderboard.md
      # ----------------------------
      - name: Update leaderboard
        env:
          TEST_LABELS_B64: ${{ secrets.TEST_LABELS_B64 }}
        run: |
          echo "Updating leaderboard..."
          python leaderboard/leaderboard_system.py || echo "Leaderboard update skipped or failed"

      # ----------------------------
      # 8. Upload leaderboard artifact
      # ----------------------------
      - name: Upload leaderboard
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: leaderboard
          path: leaderboard/leaderboard.md  # Upload the markdown leaderboard
